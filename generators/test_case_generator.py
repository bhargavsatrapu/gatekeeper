"""
Test case generator for the API Testing Agent.

This module handles the generation of comprehensive test cases for API endpoints
using LLM (Large Language Model) capabilities.
"""

import json
from typing import Dict, Any, List
import google.generativeai as genai
from config import get_config
from utils.logger import get_logger

logger = get_logger(__name__)


class TestCaseGenerator:
    """Generates comprehensive test cases for API endpoints using LLM."""
    
    def __init__(self):
        """Initialize test case generator."""
        self.config = get_config()
        genai.configure(api_key=self.config.llm.api_key)
        self.client = genai.GenerativeModel(self.config.llm.model)
        self.expected_structure = self._get_expected_test_case_structure()
    
    def _get_expected_test_case_structure(self) -> Dict[str, str]:
        """Get the expected structure for test cases."""
        return {
            "test_name": "string",
            "test_type": "positive|negative|edge|schema|auth",
            "method": "string",
            "url": "string",
            "headers": {"key": "description of expected value"},
            "query_params": {"param": "description of expected value"},
            "path_params": {"param": "description of expected value"},
            "input_payload": {"field": "description of expected value"},
            "expected_status": "string",
            "expected_schema": {"field": "description of expected type/value"}
        }
    
    def generate_test_cases(self, endpoints: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Generate comprehensive test cases for the given endpoints."""
        print(f"ü§ñ [GENERATOR] Starting test case generation for {len(endpoints)} endpoints...")
        logger.info(f"Generating test cases for {len(endpoints)} endpoints")
        try:
            print("üìù [GENERATOR] Building generation prompt...")
            prompt = self._build_generation_prompt(endpoints)
            print(f"ü§ñ [GENERATOR] Calling LLM with prompt length: {len(prompt)} characters")
            response = self.client.generate_content(prompt)
            print(f"‚úÖ [GENERATOR] LLM response received: {len(response.text)} characters")
            logger.info("Test case generation completed")
            result = self._parse_generation_response(response.text)
            
            # Handle case where no test cases were generated
            if not result or len(result) == 0:
                print("‚ö†Ô∏è [GENERATOR] No test cases generated, returning empty result")
                logger.warning("No test cases were generated by the LLM")
                return {}
            
            print(f"üìä [GENERATOR] Generated test cases for {len(result)} endpoints")
            return result
        except Exception as e:
            print(f"‚ùå [GENERATOR] Error generating test cases: {e}")
            logger.error(f"Error generating test cases: {e}")
            print("üîÑ [GENERATOR] Returning empty test cases as fallback...")
            return {}
    
    def _build_generation_prompt(self, endpoints: List[Dict[str, Any]]) -> str:
        """Build the prompt for test case generation."""
        return (
            "You are an API testing expert.\n"
            "I will provide API details in JSON format. The input may contain one or more API endpoints, "
            "including method, parameters, request body, possible responses, and response schema.\n"
            f"Here are the endpoints: {endpoints}\n\n"
            "Your task is to generate comprehensive API test cases covering:\n"
            "- Positive cases\n"
            "- Negative cases\n"
            "- Edge cases\n"
            "- Schema validation\n"
            "- Authentication and authorization (e.g., valid token, missing token, expired token, invalid token)\n\n"
            "‚ö†Ô∏è IMPORTANT RULES:\n"
            "- Do NOT insert real values like 'abc123' or 'test@example.com'.\n"
            "- For each field, provide a placeholder with its type and data source:\n"
            "   1. If the field expects RANDOM data ‚Üí describe it (e.g., 'string - random valid email').\n"
            "   2. If the field depends on another API response ‚Üí describe as "
            "'pick from an existing API response' with its type.\n\n"
            "Examples:\n"
            "  { \"username\": \"string - random valid username\" }\n"
            "  { \"email\": \"string - random valid email\" }\n"
            "  { \"token\": \"string - pick from an existing API response\" }\n"
            "  { \"user_id\": \"integer - pick from an existing API response\" }\n\n"
            "OUTPUT FORMAT:\n"
            "Return ONLY a valid JSON object. Do not include any markdown, code blocks, or explanations.\n"
            "The JSON structure should be:\n"
            "{\n"
            "  \"METHOD /path\": [\n"
            "    {\n"
            "      \"test_name\": \"string\",\n"
            "      \"test_type\": \"string\",\n"
            "      \"method\": \"string\",\n"
            "      \"url\": \"string\",\n"
            "      \"headers\": {},\n"
            "      \"query_params\": {},\n"
            "      \"path_params\": {},\n"
            "      \"input_payload\": {},\n"
            "      \"expected_status\": 200,\n"
            "      \"expected_schema\": {}\n"
            "    }\n"
            "  ]\n"
            "}\n\n"
            "CRITICAL REQUIREMENTS:\n"
            "- Return ONLY the JSON object, nothing else\n"
            "- Use double quotes for all strings\n"
            "- Ensure all JSON is properly formatted\n"
            "- No markdown code blocks (```json or ```)\n"
            "- No explanatory text before or after the JSON\n"
            "- The response must be directly parseable as JSON\n"
        )
    
    def _parse_generation_response(self, response_text: str) -> Dict[str, List[Dict[str, Any]]]:
        """Parse the LLM response and extract test cases."""
        print("üîç [GENERATOR] Parsing LLM response...")
        try:
            # Clean the response text
            cleaned_output = response_text.strip()
            
            # Remove markdown code blocks
            if cleaned_output.startswith("```"):
                lines = cleaned_output.split("\n")
                # Find the first line that doesn't start with ```
                start_idx = 0
                for i, line in enumerate(lines):
                    if not line.strip().startswith("```"):
                        start_idx = i
                        break
                # Find the last line that doesn't end with ```
                end_idx = len(lines)
                for i in range(len(lines) - 1, -1, -1):
                    if not lines[i].strip().endswith("```"):
                        end_idx = i + 1
                        break
                cleaned_output = "\n".join(lines[start_idx:end_idx]).strip()
            
            # Remove any remaining code block markers
            cleaned_output = cleaned_output.strip("`").strip()
            
            # Try to find JSON content between curly braces
            if "{" in cleaned_output and "}" in cleaned_output:
                start_idx = cleaned_output.find("{")
                end_idx = cleaned_output.rfind("}") + 1
                cleaned_output = cleaned_output[start_idx:end_idx]
            
            print(f"üìù [GENERATOR] Cleaned response length: {len(cleaned_output)} characters")
            logger.debug(f"Cleaned LLM response: {cleaned_output[:200]}...")
            
            # Try to parse the JSON
            test_cases = json.loads(cleaned_output)
            
            if not isinstance(test_cases, dict):
                raise ValueError("LLM response is not a dictionary")
            
            # Validate the structure
            for endpoint, cases in test_cases.items():
                if not isinstance(cases, list):
                    print(f"‚ö†Ô∏è [GENERATOR] Warning: Test cases for {endpoint} is not a list, skipping...")
                    continue
                for case in cases:
                    if not isinstance(case, dict):
                        print(f"‚ö†Ô∏è [GENERATOR] Warning: Test case in {endpoint} is not a dictionary, skipping...")
                        continue
            
            total_cases = sum(len(cases) for cases in test_cases.values())
            print(f"‚úÖ [GENERATOR] Successfully parsed {total_cases} test cases for {len(test_cases)} endpoints")
            logger.info(f"Successfully parsed {total_cases} test cases")
            return test_cases
            
        except json.JSONDecodeError as e:
            print(f"‚ùå [GENERATOR] Invalid JSON in LLM response: {e}")
            print(f"üìù [GENERATOR] Raw response: {response_text[:500]}...")
            print(f"üìù [GENERATOR] Cleaned response: {cleaned_output[:500]}...")
            logger.error(f"Invalid JSON in LLM response: {e}")
            logger.error(f"Raw response: {response_text}")
            logger.error(f"Cleaned response: {cleaned_output}")
            
            # Try to provide a fallback empty response
            print("üîÑ [GENERATOR] Attempting to return empty test cases as fallback...")
            return {}
            
        except Exception as e:
            print(f"‚ùå [GENERATOR] Error parsing generation response: {e}")
            logger.error(f"Error parsing generation response: {e}")
            print("üîÑ [GENERATOR] Attempting to return empty test cases as fallback...")
            return {}


class TestCaseValidator:
    """Validates generated test cases for quality and completeness."""
    
    def __init__(self):
        """Initialize test case validator."""
        self.config = get_config()
        genai.configure(api_key=self.config.llm.api_key)
        self.client = genai.GenerativeModel(self.config.llm.model)
    
    def validate_test_cases(self, test_cases: Dict[str, List[Dict[str, Any]]]) -> str:
        """Validate generated test cases using LLM."""
        logger.info("Validating generated test cases")
        try:
            prompt = self._build_validation_prompt(test_cases)
            response = self.client.generate_content(prompt)
            feedback = self._parse_validation_response(response.text)
            logger.info("Test case validation completed")
            return feedback
        except Exception as e:
            logger.error(f"Error validating test cases: {e}")
            return "Validation failed due to an error"
    
    def _build_validation_prompt(self, test_cases: Dict[str, List[Dict[str, Any]]]) -> str:
        """Build the prompt for test case validation."""
        return (
            "You are a senior QA reviewer.\n"
            "Review the following generated API test cases:\n"
            f"{json.dumps(test_cases, indent=2)}\n\n"
            "Evaluation criteria:\n"
            "- All positive, negative, edge, and schema validation cases must be covered.\n"
            "- Testcases must be logically valid and executable.\n"
            "- IMPORTANT: No actual values should be used. All fields (headers, query_params, "
            "path_params, input_payload, expected_schema) must contain descriptive placeholders.\n"
            "  ‚úÖ Example: { \"email\": \"string - valid email address\" }\n"
            "  ‚ùå Wrong:   { \"email\": \"test@example.com\" }\n\n"
            "Respond ONLY with JSON in this format:\n"
            "{ \"feedback\": \"Your feedback text\" }"
        )
    
    def _parse_validation_response(self, response_text: str) -> str:
        """Parse the validation response and extract feedback."""
        try:
            cleaned_output = response_text.strip("`").strip()
            if cleaned_output.startswith(("json", "python")):
                cleaned_output = cleaned_output.split("\n", 1)[-1].strip()
            result = json.loads(cleaned_output)
            feedback = result.get("feedback", "No feedback provided")
            logger.info(f"Validation feedback: {feedback[:100]}...")
            return feedback
        except json.JSONDecodeError:
            logger.warning("Invalid JSON in validation response, using raw text")
            return response_text
        except Exception as e:
            logger.error(f"Error parsing validation response: {e}")
            return "Validation response parsing failed"
    
    def should_regenerate(self, feedback: str) -> bool:
        """Determine if test cases should be regenerated based on feedback."""
        negative_keywords = [
            "actual value",
            "real value",
            "specific value",
            "hardcoded",
            "concrete value"
        ]
        feedback_lower = feedback.lower()
        should_regenerate = any(keyword in feedback_lower for keyword in negative_keywords)
        if should_regenerate:
            logger.info("Regeneration needed based on validation feedback")
        else:
            logger.info("Test cases passed validation")
        return should_regenerate


# Global instances
test_case_generator = TestCaseGenerator()
test_case_validator = TestCaseValidator()


def get_test_case_generator() -> TestCaseGenerator:
    """Get the global test case generator instance."""
    return test_case_generator


def get_test_case_validator() -> TestCaseValidator:
    """Get the global test case validator instance."""
    return test_case_validator
